version: '3.8'

services:
  lm-evaluation-harness:
    image: lm-evaluation-harness
    container_name: gplsi_lm-evaluation-harness
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - WANDB_API_KEY=${WANDB_API_KEY}
      - MODEL_ID_HUGGING_FACE=${MODEL_ID_HUGGING_FACE} 
      - HF_TOKEN=${HF_TOKEN}
      - WANDB_PROJECT=$WANDB_PROJECT
    network_mode: "host"  # Use host network (same as --network="host" in docker run)
    stdin_open: true  # Keeps the stdin open (interactive mode)
    tty: true  # Allocate a pseudo-TTY (interactive terminal)
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [gpu]
              device_ids: ["all"]  # Let the container use all GPUs

    volumes:
    - ./outputLogs/:/outputLogs
    - ./results/:/app/results
    - ./reports/:/app/reports
    #- /home/NAS/GPLSI/:/home/NAS/GPLSI
