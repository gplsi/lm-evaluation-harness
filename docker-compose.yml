version: '3.8'

services:
  lm-evaluation-harness:
    image: lm-evaluation-harness
    container_name: gplsi_lm-evaluation-harness
    environment:
      - NVIDIA_VISIBLE_DEVICES=all  # Make all GPUs visible  # Specify the GPUs you want to use (GPU 0)
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility  # Enable GPU capabilities
      - WANDB_API_KEY=${WANDB_API_KEY} # Pass the API key dynamically
      - MODEL_ID_HUGGING_FACE=${MODEL_ID_HUGGING_FACE} # Pass the MODELNAME dynamically
      - WANDB_PROJECT=$WANDB_PROJECT
    network_mode: "host"  # Use host network (same as --network="host" in docker run)
    stdin_open: true  # Keeps the stdin open (interactive mode)
    tty: true  # Allocate a pseudo-TTY (interactive terminal)
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [gpu]
              device_ids: ["all"]  # Let the container use all GPUs

    volumes:
    - ./outputLogs/:/outputLogs
    - ./results/:/app/results
    - ./reports/:/app/reports
